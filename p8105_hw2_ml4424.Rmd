---
title: "p8105_hw2_ml4424"
author: "Maggie Li (ml4424)"
date: "9/28/2020"
output: github_document
---
## Read in libraries

```{r}
library(readxl)
library(tidyverse)
```


## Problem 1

```{r read and clean mr trash w}
trash = read_excel("prob1_data/Trash-Wheel-Collection-Totals-8-6-19.xlsx", 
                   sheet = "Mr. Trash Wheel",
                   cell_cols("A:N")) %>% 
  janitor::clean_names() %>% 
  drop_na(year) %>% #drop rows with NAs that are just totals; not dumpster-specific
  mutate(rnd_sb = as.integer(round(sports_balls, 0)))
  
trash
```

```{r read and clean precip 2017 2018}
precip_17 = read_excel("prob1_data/Trash-Wheel-Collection-Totals-8-6-19.xlsx", 
                   sheet = "2017 Precipitation",
                   skip = 1) %>% 
  janitor::clean_names() %>% 
  drop_na(month) %>% 
  mutate(year = 2017)
  
precip_17

precip_18 = read_excel("prob1_data/Trash-Wheel-Collection-Totals-8-6-19.xlsx", 
                   sheet = "2018 Precipitation",
                   skip = 1) %>% 
  janitor::clean_names() %>% 
  drop_na(month) %>% 
  mutate(year = 2018)
  
precip_18

# bind tbls
precip_17_18 <- bind_rows(precip_17, precip_18) %>%
  mutate(month = month.name[month])
precip_17_18

# total precip in 2018
tot_precip_18 <- precip_18 %>% summarize(sum(total))

# median sports balls in dumpster in 2017 (i assume we are pulling from the rounded sports ball column)
median_sb <- trash %>% filter(year == 2017) %>%
  summarize(median(rnd_sb))
median_sb
```
**Description of Mr. Trash Wheel and Precipitation Data:** The Mr. Trash Wheel data are formatted so that each entry specifies a dumpster filled by the vessel, including the date which a full dumpster of trash was collected and columns with details on the waste weight, volume and materials composition, as well as homes powered when the dumpster is transported to a waste-to-energy plant to create electricity. There are 344 observations, meaning there have been 344 dumpsters filled by the contraption between May 2014 and June 2019.

The precipitation data for 2017 and 2018 contain monthly precipitation (inches of rainfall). There are 24 observations in the data, one row for every month in these two years. The total precipitation was `r tot_precip_18` inches. The mean number of sports balls collected in a dumpster in 2017 was `r median_sb`.


## Problem 2

```{r read and clean NYC transit data}
transit_dta <- read_csv("prob2_data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>% 
  janitor::clean_names() %>% 
  dplyr::select(line, station_name, station_latitude, station_longitude, starts_with("route"),
                entry, vending, entrance_type, ada) %>% # select relevant p columns
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE)) # logical vector
transit_dta
```

**Description:** These data describe NYC subway station characteristics. In the cleaned data, each row represents a unique subway station's entrance or exit in NYC. From the raw data, we have kept relevant identifying information on the subway line served, the name of the station, the latitude and longitude coordinates, the routes served by the specific station, whether or not a specific entrance/exit allows entry or is just exit-only, the mode of entrance into the station, and whether or not it is ADA compliant. We further cleaned the data by converting entry to a logical T/F given whether or not the entrance allows entry (TRUE) or is exit-only (FALSE). The dimensions are 1,868 rows by 19 columns. These data are not tidy; the route information is spread across 11 columns.


```{r answer following questions using these data}
# distinct stations
distinct_stn <- distinct(transit_dta, line, station_name) %>% nrow()

# number of ADA compliant stations (up one level from entrance/exits at stations)
ADA_comp <- transit_dta %>% 
  select(line, station_name, ada) %>% # drop all other columns of station sublevels, only interested in station-level attributes
  distinct() %>% 
  filter(ada == T) %>% nrow()
ADA_comp

# proportion of entrance/exits without vending allow entrance
entry_no_vend <- transit_dta %>% 
  filter(vending == "NO", 
         entry == T) %>% nrow()
```

There are `r distinct_stn` distinct stations. Out of these stations, `r ADA_comp` are ADA-compliant. Of all station entrance/exits without vending, `r entry_no_vend` allow entrance.

```{r reformat so route number and name are distinct vars; A train questions}
tidy_transit <- transit_dta %>% 
  mutate_if(is.numeric, as.character) %>%  # convert numeric route columns to characters to avoid errors later
  pivot_longer(route1:route11,
               names_to = "route",
               values_to = "route_name") %>%
  drop_na() %>%  # drop NA rows, these are extra rows that don't contain route at specific station entrance/exit when we tidied data
  distinct(line, station_name, route_name,
           .keep_all = TRUE) # pull distinct line, station_name, and routes at these stations
tidy_transit


# how many distinct stations serve the A train
A_stns <- tidy_transit %>% filter(route_name == "A") 

# ADA compliant A train stations
ADA_A_stns <- A_stns %>% filter(ada == TRUE) %>% nrow()
ADA_A_stns
```

There are `r A_stns %>% nrow()` that serve the A train. Of these stations, `r ADA_A_stns` are ADA compliant.

## Problem 3

```{r clean pols-month data}
pols <- read_csv("prob3_data/fivethirtyeight_datasets/pols-month.csv") %>% 
  janitor::clean_names() %>% 
  separate(mon, c("year", "month", "date")) %>%
  mutate(month = month.name[as.integer(month)]) %>% # replace month number with month name
  pivot_longer(cols = starts_with("prez"),
               names_to = "president",
               values_to = "party",
               names_prefix = "prez_") %>% # wide to long format for president & party
  filter(party >= 1) %>% # we've duplicated each month/year combination, so we want to get rid of duplicates                           
                          # (party = 0); i.e. keep 1 (yes) and weird 2 values during Watergate(?)
  select(-date, -party) # only keep year and month as leading key cols
pols
```

```{r clean snp.csv file}
snp <- read_csv("prob3_data/fivethirtyeight_datasets/snp.csv") %>% 
  janitor::clean_names() %>% 
  separate(date, c("month", "date", "year")) %>%
  select(year, month, close) %>%
  mutate(month = month.name[as.integer(month)])
snp
```

```{r tidy unemploy dta}
unemployment <- read_csv("prob3_data/fivethirtyeight_datasets/unemployment.csv") %>% 
  janitor::clean_names() %>%
  rename_at(vars(tolower(month.abb)), ~month.name) %>% 
  pivot_longer(January:December,
               names_to = "month") %>% 
  mutate(year = as.character(year)) # coerce year to character to match previous datasets
unemployment
```

```{r join data}
# merge snp to pols and merge unemployment into this
joined_dta <- left_join(pols, snp) %>% left_join(unemployment)
joined_dta
```

**Description**: The pols dataset contains 822 rows for each month in every year between January 1947 and June 2015 (68 years * 12 months + 6 months), and several columns for number of republican or democratic governors, national senators and representatives, and two columns signifying whether or not the sitting president was dem or gop (1/0) that month. We mutated the data so that *president* was the identifying column, and dem or gop were the potential values for this column. The final dimensions were 822 x 9.

The snp data contain 787 rows for each month in every year between January 1950 and July 2015 (65 years * 12 months + 7 months), and a column indicating the closing values of the S&P stock on a specified date during the observation month, between the 1st and 4th date of the month. We transformed the data so it would be in the same format as the pols dataset, parsing out the *date* column into *year* and *month* and replacing month number with month name. The final dimensions were 787 x 3.

The final unemployment data contains the unemployment rate for every month between January 1948 and December 2015. In the original data, there were 68 rows for each of these 68 years, with columns indicating the unemployment rate for each month of the year. We tidied this data so that every row instead was a month of a given year and the final dimension was n = 816 (68 years * 12 months), and the *value* column indicated that monthly unemployment rate. The final dimensions were 816 x 3.

We were able to join all three datasets on key variables *year* and *month*. In the final joined dataset, we had 822 rows from the pols dataset, so every row represented a month within the study period between January 1947 and June 2015, with additional columns for monthly S&P stock data and unemployment rate which we joined to the pols data. There are NA values for the months that were missing S&P and/or unemployment data in 1947-1949.
